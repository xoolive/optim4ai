# Further reading

[« Previous](../8_evolution) \| [Home ↑](../)

We are constrained by time in this course, and there is a lot more to learn about in terms of optimisation methods. This page lists many references to important keywords in this field.

## Gradient descent methods

- Learn how to solve quadratic problems with SLSQP;
- Learn about quasi-Newton methods for fast gradient converging methods like BFGS;
- Learn about constrained optimisation with the Karush-Kuhn-Tucker (KKT) theorem.

## Optimisation for Machine Learning

- Learn about common optimisation methods for ML, e.g. ADAM

## Linear programming

- Learn about interior-point methods;
- Learn about the duality theorem

## Mixed integer linear programming

- Learn about common ways to linearise constraints;
- Learn about more advanced cuts, e.g. Gomory cuts;
- Learn about advanced modelling tricks, like Benders' decomposition or Dantzig-Wolfe decomposition

## A quick introduction to complexity

## Constraint programming

- Learn about more propagation mechanisms for global constraints;
- Learn about local search and it fits CP for large-neighbourhood search;
- Learn about SAT and SMT theories

## Evolutionary methods

- Learn about more metaheuristics such as Particle Swarm Optimisation, Ant Colony Optimisation, and more;
- Learn about genetic programming, and see how we can make programs, graphs and networks evolve;
- Learn how evolutionary methods can help optimise the training of neural networks hyperparameters
